{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4654a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T16:15:44.543536Z",
     "start_time": "2023-04-25T16:15:44.191655Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1962a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db641fc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "最简单最基本的模型，$\\hat{y}=x*\\omega+b $\n",
    "\n",
    "定义损失函数为：$loss=\\sum(\\hat{y}-y)^2=\\sum(x*\\omega-y)^2$\n",
    "\n",
    "对不同的$\\omega$进行遍历，算出损失，找到最小的$\\omega$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9d1ce",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "    \n",
    "def loss(x, y):\n",
    "    return (forward(x) - y) ** 2\n",
    "\n",
    "w_list = []\n",
    "mse_list = []\n",
    "for w in np.arange(0.0, 4.0, 0.1):\n",
    "    # print('w=', w)\n",
    "    l_sum = 0.0\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        loss_val = loss(x_val, y_val)\n",
    "        l_sum += loss_val\n",
    "        # print('\\t', x_val, y_val, y_pred_val, loss_val)\n",
    "    # print('MSE=', l_sum / len(x_data))\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum / 3)\n",
    "    \n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a036ecc",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": true
   },
   "source": [
    "# 梯度下降算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bde97d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src = './Image/Pytorch深度学习实践/梯度下降.png' width = 400 align = right>\n",
    "对于函数而言，梯度有$\\nabla f = \\frac{\\partial f}{\\partial x}$\n",
    "\n",
    "对于当前的$\\omega$，如果梯度大于0，说明误差在增大，否则误差在减小，故下一个取得的$\\omega$为：\n",
    "\n",
    "$\\omega = \\omega - \\alpha * \\frac{\\partial f}{\\partial x}$，其中$\\alpha$参数称作学习率(学习率不能太大，否则会直接越过最低点)\n",
    "\n",
    "梯度下降实际上是一种贪心算法，它得到的是局部区域的一个最优结果，如果损失函数不是一个“凸函数”，可能得不到最好的结果\n",
    "\n",
    "现实中，太多的局部最优解其实很少见，更常见的是“鞍点”——梯度向量为0的点，在二维平面里，会出现无法继续迭代的现象，在三维即更高的空间里，可能出现从一个面看是最高点，而另一个面是最低点\n",
    "\n",
    "<img src = './Image/Pytorch深度学习实践/鞍点.jpg' width = 400>\n",
    "\n",
    "在cost函数中：\n",
    "$$\\frac{\\partial cost(\\omega)}{\\partial \\omega} = \\frac{\\partial}{\\partial \\omega} \\frac{1}{N}\\sum_{n=1}^N(x_n*\\omega-y_n)^2 = \\frac{1}{N}\\sum_{n=1}^N2x_n(x_n*w-y_n)\n",
    "$$\n",
    "则\n",
    "$$\\omega = \\omega - \\alpha * \\frac{\\partial f}{\\partial x} = \\omega - \\alpha * \\frac{1}{N}\\sum_{n=1}^N2x_n(x_n*w-y_n)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d188a04f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-22T03:31:37.741869Z",
     "start_time": "2023-03-22T03:31:36.494025Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 梯度下降算法\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# prepare the training set\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "# initial guess of weight\n",
    "w = 1.0\n",
    "\n",
    "\n",
    "# define the model linear model y = w*x\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "#define the cost function MSE\n",
    "def cost(xs, ys):\n",
    "    cost = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        y_pred = forward(x)\n",
    "        cost += (y_pred - y)**2\n",
    "    return cost / len(xs)\n",
    "\n",
    "\n",
    "# define the gradient function  gd\n",
    "def gradient(xs, ys):\n",
    "    grad = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        grad += 2 * x * (x * w - y)\n",
    "    return grad / len(xs)\n",
    "\n",
    "\n",
    "epoch_list = []\n",
    "cost_list = []\n",
    "# print('predict (before training)', 4, forward(4))\n",
    "for epoch in range(100):\n",
    "    cost_val = cost(x_data, y_data)\n",
    "    grad_val = gradient(x_data, y_data)\n",
    "    w -= 0.01 * grad_val  # 0.01 learning rate\n",
    "    # print('epoch:', epoch, 'w=', w, 'loss=', cost_val)\n",
    "    epoch_list.append(epoch)\n",
    "    cost_list.append(cost_val)\n",
    "\n",
    "# print('predict (after training)', 4, forward(4))\n",
    "plt.plot(epoch_list, cost_list)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16511e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 随机梯度下降算法\n",
    "不采取整体的cost的梯度，而采取随机一个样本的loss的梯度，避免整体的梯度为0带来的鞍点导致无法更新$\\omega$\n",
    "+ 优点：避免了鞍点，性能好\n",
    "+ 缺点：整体梯度下降可以采用并行计算最后汇总，而随机算法因为每次都会迭代更新$\\omega$，如果要达到同样的训练此数，只能用循环 时间慢\n",
    "\n",
    "解决方法：\n",
    "+ 将原数据(batch)分组(mini_batch)，对每组用整体计算cost，然后随机挑选一组的cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1991f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 随机梯度下降算法\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# prepare the training set\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "# initial guess of weight\n",
    "w = 1.0\n",
    "\n",
    "\n",
    "# define the model linear model y = w*x\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "#define the loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)**2\n",
    "\n",
    "\n",
    "# define the gradient function  gd\n",
    "def gradient(x, y):\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "# print('predict (before training)', 4, forward(4))\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        grad_val = gradient(x, y)\n",
    "        w -= 0.01 * grad_val  # 0.01 learning rate\n",
    "        # print('epoch:', epoch, 'w=', w, 'loss=', cost_val)\n",
    "\n",
    "    epoch_list.append(epoch)\n",
    "    loss_list.append(l)\n",
    "\n",
    "# print('predict (after training)', 4, forward(4))\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e564db1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 反向传播与pytorch初步实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76d661",
   "metadata": {
    "hidden": true
   },
   "source": [
    "反向传播的一个简要解析：[梯度反向传播简解](https://zhuanlan.zhihu.com/p/40378224)\n",
    "<img src = './Image/Pytorch深度学习实践/反向传播.png' width = 300>\n",
    "结合上图，我们的预测值$y$是一个关于输入的函数：$\\hat{y}=f_\\omega(\\pmb{x})$，梯度下降算法通过对$loss = (\\hat{y}-y)^2$求梯度，根据梯度修正参数$\\omega$的值直到梯度/导数收敛为0，此时我们可以认为找到了最小的$loss$\n",
    "\n",
    "问题的关键是如何对每个参数都求得对应的梯度，反向传播的思想为先正向算得中间量(forward过程)，然后利用链式法则，反向计算每个参数的梯度(Back Propagation过程)，前面的中间量可以带入到链式过程中简化运算；\n",
    "\n",
    "Pytorch中以张量$Tensor(data,grad)$为基本类型(grad也为张量类型)，在构造前向图的时候，梯度信息会自动保留，当前向计算完毕，释放最终结果时，沿着计算图在Tensor里的grad属性即会保留在这次运算中的梯度值\n",
    "\n",
    "还是以$y=\\omega x$为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f668f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-22T03:32:50.226006Z",
     "start_time": "2023-03-22T03:32:50.025833Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "# w即为需要球梯度的张量\n",
    "w = torch.Tensor([1.0])\n",
    "w.requires_grad = True\n",
    "\n",
    "\n",
    "def forword(x):\n",
    "    # w为Tensor,最后的return会自动类型转换为Tensor\n",
    "    return x * w\n",
    "\n",
    "# 每次调用loss函数都会构造出一个计算图\n",
    "def loss(x, y):\n",
    "    y_pred = forword(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        w.grad.data.zero_()\n",
    "    loss_list.append(l.item())\n",
    "    epoch_list.append(epoch)\n",
    "\n",
    "# print(loss_list)\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654672f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "课后作业：假设推测的模型为$\\hat{y}=\\omega_1x^2+\\omega_2x+b$，估计参数(三个方程，三个未知数的求解)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d14050",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "# w即为需要球梯度的张量\n",
    "w1 = torch.Tensor([1.0])\n",
    "w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0])\n",
    "w2.requires_grad = True\n",
    "b = torch.Tensor([1.0])\n",
    "b.requires_grad = True\n",
    "\n",
    "\n",
    "def forword(x):\n",
    "    # w为Tensor,最后的return会自动类型转换为Tensor\n",
    "    return w1 * x * x + w2 * x + b\n",
    "\n",
    "\n",
    "# 每次调用loss函数都会构造出一个计算图\n",
    "def loss(x, y):\n",
    "    y_pred = forword(x)\n",
    "    return (y_pred - y)**2\n",
    "\n",
    "\n",
    "l_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        l = loss(x, y)\n",
    "        l.backward()\n",
    "        w1.data = w1.data - 0.01 * w1.grad.data\n",
    "        w2.data = w2.data - 0.01 * w2.grad.data\n",
    "        b.data = b.data - 0.01 * b.grad.data\n",
    "        w1.grad.data.zero_()\n",
    "        w2.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    l_list.append(l.item())\n",
    "    epoch_list.append(epoch)\n",
    "\n",
    "# print(l_list)\n",
    "print(w1.item(),w2.item(),b.item(),forword(4).item())\n",
    "plt.plot(epoch_list, l_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cefe169",
   "metadata": {},
   "source": [
    "# pytorch运用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2640515",
   "metadata": {},
   "source": [
    "Pytorch的过程为：准备数据集->设计模型->构造损失函数(loss)和优化函数(例如梯度优化，0.01的步长)->循环优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1491ab6c",
   "metadata": {},
   "source": [
    "+ 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9513ce0b",
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575dff96",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "+ 构造模型，Pytorch里的模型都是继承于*torch.nn.Module*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0937592",
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    #调用父类方法\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Linear本身也是一个Module的子类。可以实例化对象\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    # 重载父类forward()函数\n",
    "    def forward(self, x):\n",
    "        #由子类实例化出的对象可以使用()重载\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f040c",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "&emsp;*torch.nn.Linear*的两个参数表示输入的维数和输出的维数例如有n个样本，输入$x$为三维($3\\times n$)，输出$\\hat{y}$为两维($2\\times n$)，则参数矩阵$\\omega^T$为$\\begin{bmatrix}\\cdots\\end{bmatrix}_{2\\times 3},\\hat{y}=\\omega^Tx$，另外还有默认参数：*bias=True*, 即 ***torch.nn.Linear(in_features, out_features, bias=True)***\n",
    "\n",
    "&emsp;对于调用：\"*y_pred = self.linear(x)*\" 此处实际上是重载了类的*()* 符（类似C++里的*void operator()(...)* ），在父类里重写了*__call__* 方法，并由子类继承，可以由实例化对象直接调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f31b5c",
   "metadata": {},
   "source": [
    "+ 构造损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e474d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSEloss也继承于torch.nn.Module,参数reduction指定最后的loss形式,前两个参数为y'和y\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# 来自optim模块,SGD是一个优化器类,第一个参数传递需要求梯度的参数,第二个是步长\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "optimizer1 = torch.optim.Rprop(model.parameters(), lr=0.01)\n",
    "\n",
    "optimizer2 = torch.optim.Adamax(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f999b86",
   "metadata": {},
   "source": [
    "&emsp;model会在父类里继承$parameters$方法，告知SGD需要反向传播求梯度的参数，此处实际上就是$Linear$类构造函数里传入的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c10b9",
   "metadata": {},
   "source": [
    "+ 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf057d",
   "metadata": {
    "code_folding": [
     15,
     25
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_list = []\n",
    "loss_list = []\n",
    "loss_list1 = []\n",
    "loss_list2 = []\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    loss_list.append(loss.item())\n",
    "    epoch_list.append(epoch)\n",
    "    # 使用optimizer会收集参数\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 自动根据梯度更新权重\n",
    "    optimizer.step()\n",
    "    \n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    loss_list1.append(loss.item())\n",
    "    # 使用optimizer会收集参数\n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    # 自动根据梯度更新权重\n",
    "    optimizer1.step()\n",
    "    \n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    loss_list2.append(loss.item())\n",
    "    # 使用optimizer会收集参数\n",
    "    optimizer2.zero_grad()\n",
    "    loss.backward()\n",
    "    # 自动根据梯度更新权重\n",
    "    optimizer2.step()\n",
    "    \n",
    "print('w= ', model.linear.weight.item())\n",
    "print('b= ', model.linear.bias.item())\n",
    "print(\"最后的成品图会受到pytorch自动给定的参数的影响\")\n",
    "print(\"可以看到,按照不同方式优化的结果,收敛速度不同,并且有可能出现过拟合情况/次数多了反而发散\")\n",
    "\n",
    "#设置中文字符\n",
    "plt.rcParams['font.family'] = 'SimHei'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "#为子图设置合适的大小\n",
    "plt.figure(figsize=(6,9))\n",
    "\n",
    "#为每个模型构造子图\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.title(\"SGD优化\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(epoch_list, loss_list1)\n",
    "plt.title(\"Rprop优化\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(epoch_list, loss_list2)\n",
    "plt.title(\"Adamax优化\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "\n",
    "#优化间距\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1a1cd",
   "metadata": {},
   "source": [
    "最后的w和b是Tensor类型，需要使用*.item()* 打印数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410668b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 逻辑斯蒂回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff58173",
   "metadata": {
    "hidden": true
   },
   "source": [
    "&emsp;考虑分类问题，将一张手写图片映射到$\\mathbb{y}=\\{0,1,2,3,4,5,6,7,8,9\\}$上，如果考虑用回归的方式处理，注意到这时候输入集和输出集上并没有大小关系的序，而只是不同的类；我们关注的不是某个输入的输出值，而是对每个可能取值的概率\n",
    "\n",
    "&emsp;pytorch提供了一些数据集：MNIST(手写数字)、CIFAR10(图片分类)\n",
    "\n",
    "&emsp;在之前的问题中，可以以$y$的大小分为{fail;pass}(二分问题)，我们需要将输入映射到$0\\rightarrow 1$的概率输出上(Sigmoid函数/饱和函数)，常用的函数为：$\\sigma(x)=\\frac{1}{1+e^{-x}}$，$\\sigma'(x)=\\frac{e^x}{(1+e^x)^2}$特点：有正负极限、单调增、饱和(在正负无穷导数为0，在0处导数max)，对于上述线性模型有：\n",
    "\n",
    "$$\\hat{y}=\\sigma(\\omega x+b)$$\n",
    "\n",
    "&emsp;此处$\\hat y$表示y取1的概率：$\\begin{cases}\\begin{align}&\\hat y&&P(CLASS=1) \\\\ &1-\\hat y&&P(CLASS=0)\\end{align}\\end{cases}$，不能继续用$loss=(\\hat y - y)^2$计算损失，此处使用交叉熵来衡量离散分布的损失：\n",
    "\n",
    "$$loss=-\\frac{1}{N}\\sum_{i=1}^{N}p_{M1}(x=i)logp_{M2}(x=i)\\quad p_{M1}为样本的实际值—0或1,p_{M2}为模型输出的概率$$\n",
    "\n",
    "&emsp;loss加负号使得值越小越好，当二分时，$loss=-(ylog\\hat{y}+(1-y)log(1-\\hat y))=\\begin{cases}\\begin{align}&-log\\hat y,&&when\\quad y=1,\\quad \\hat y \\mathop{\\longrightarrow}\\limits^{getMax} 1 \\quad \\\\ &-log(1-\\hat y),&&when\\quad y=0,\\quad \\hat y \\mathop{\\longrightarrow}\\limits^{getMax} 0 \\end{align}\\end{cases}$——*BCE Loss*\n",
    "<img src='./Image/Pytorch深度学习实践/BCE损失.png' width = 300>\n",
    "\n",
    "*(值得说明的是，之前的线性模型计算的loss其实也不是方差，它是每一个估计量相当于实际值的偏差，而方差为实际值之间的平均分布的衡量)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87936f8",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#准备数据集\n",
    "#-------------------------------------------------------------------------------------------\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[0.0], [0.0], [1.0]])\n",
    "\n",
    "\n",
    "#构建模型\n",
    "#-------------------------------------------------------------------------------------------\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    #调用父类方法\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Linear本身也是一个Module的子类。可以实例化对象\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    # 重载父类forward()函数\n",
    "    def forward(self, x):\n",
    "        #由子类实例化出的对象可以使用()重载\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "model = LogisticRegressionModel()\n",
    "\n",
    "\n",
    "#设置损失函数和优化器\n",
    "#-------------------------------------------------------------------------------------------\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "# 来自optim模块,SGD是一个优化器类,第一个参数传递需要求梯度的参数,第二个是步长\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "#循环优化\n",
    "#-------------------------------------------------------------------------------------------\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "for epoch in range(10000):\n",
    "    y_pred = model.forward(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    loss_list.append(loss.item())\n",
    "    epoch_list.append(epoch)\n",
    "    # 使用optimizer会收集参数\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 自动根据梯度更新权重\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "plt.rcParams['font.family'] = 'SimHei'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print('w= ', model.linear.weight.item())\n",
    "print('b= ', model.linear.bias.item())\n",
    "print(\"x = 4.0时的预测为: \", model.forward(torch.Tensor([4.0])).item())\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.title(\"损失函数\")\n",
    "\n",
    "x = np.arange(0, 10, 0.05)\n",
    "# 利用view将矩阵变成200行一列\n",
    "x_t = torch.Tensor(x).view((200, 1))\n",
    "y_t = model(x_t)\n",
    "# view(-1)表示线性化, 即将矩阵变成1行\n",
    "y = y_t.data.view(-1).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y)\n",
    "plt.ylabel(r'$\\hat{y}$')\n",
    "plt.xlabel('x')\n",
    "plt.grid()\n",
    "plt.title(\"模型预测\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d623fc",
   "metadata": {},
   "source": [
    "# 多维特征输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6e98b",
   "metadata": {},
   "source": [
    "<img src='./Image/Pytorch深度学习实践/多维输入.png' width = 500>\n",
    "\n",
    "对于上述的分类任务，每一行称为一个**sample**/样本，每一列称为一个**feature**/特征，在该表中，一个样本有八个特征，对于上述的模型有：\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)}=\\sigma(\\sum_{n=1}^8\\omega_nx_n^{(i)}+b)=\\sigma(\\pmb w^T\\pmb x^{(i)}+b)=\\sigma(z^{(i)}),\\omega和x均为列向量,z^{(i)}为第i个样本线性计算出的值 \n",
    "\\\\\n",
    "\\begin{bmatrix} \\hat y^{(1)} \\\\ \\vdots \\\\ \\hat y^{(N)} \\end{bmatrix} = \n",
    "\\begin{bmatrix} \\sigma(z^{(1)}) \\\\ \\vdots \\\\ \\sigma(z^{(N)}) \\end{bmatrix} = \n",
    "\\sigma \\left( \\begin{matrix} z^{(1)} \\\\ \\vdots \\\\ z^{(N)} \\end{matrix} \\right),Tensor和numpy一样提供广播\n",
    "\\\\ \n",
    "\\begin{bmatrix} z^{(1)} \\\\ \\vdots \\\\ z^{(N)} \\end{bmatrix}_{N\\times 1} = \n",
    "\\begin{bmatrix}x_1^{(1)}& \\cdots & x_8^{(1)}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_N^{(1)}& \\cdots & x_N^{(8)} \\end{bmatrix}_{N\\times 8}\n",
    "\\begin{pmatrix}\\omega_1 \\\\ \\vdots \\\\ \\omega_8 \\end{pmatrix}_{8\\times 1} + \n",
    "\\begin{pmatrix}b \\\\ \\vdots \\\\ b \\end{pmatrix},8维N个样本到1维N个目标的映射\n",
    "$$\n",
    "\n",
    "我们的输入为8维，输出仍然为1维，此时使用的变换矩阵维为$[\\cdots]_{8\\times 1}$我们可以叠加网络层：$[\\cdots]_{8\\times 2}\\times [\\cdots]_{2\\times 1}$先将8维映射到2维，再将2维映射到1维，为了避免每一个线性变换叠加后仍为线性，为每一个线性变换添加非线性因子——**激活函数**，使得最后为非线性\n",
    "<img src='./Image/Pytorch深度学习实践/多维映射.png' width = 500>\n",
    "<img src='./Image/Pytorch深度学习实践/多维线性模型.png' width = 400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab6117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T14:26:02.800150Z",
     "start_time": "2023-03-25T14:23:32.905037Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pylab as plt \n",
    "\n",
    "#使用gpu加速，所有相关的运算都要放在gpu上\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \", device)\n",
    "\n",
    "#准备数据集\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# delimiter说明数据间的分隔符\n",
    "xy = np.loadtxt(\"./dataset/pytorch深度学习入门/diabetes.csv.gz\", delimiter=',', dtype=np.float32)\n",
    "\n",
    "#取除了最后一列的数据\n",
    "x_data = torch.tensor(xy[:,:-1])\n",
    "# x_data = torch.from_numpy(xy[:,:-1])\n",
    "\n",
    "#取最后一列的数据, 注意到y_data是个二维的矩阵 \n",
    "y_data = torch.tensor(xy[:,-1:])\n",
    "\n",
    "#构建模型\n",
    "#-------------------------------------------------------------------------------------------\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 2)\n",
    "        self.linear4 = torch.nn.Linear(2, 1)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.Relu(self.linear1(x))\n",
    "        x = self.Relu(self.linear2(x))\n",
    "        x = self.Relu(self.linear3(x))\n",
    "        x = self.sigmoid(self.linear4(x))\n",
    "        return x\n",
    "    \n",
    "model = Model()\n",
    "model.to(device)\n",
    "\n",
    "#设置损失函数和优化器\n",
    "#-------------------------------------------------------------------------------------------\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "# 来自optim模块,SGD是一个优化器类,第一个参数传递需要求梯度的参数,第二个是步长\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "#循环优化\n",
    "#-------------------------------------------------------------------------------------------\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "x_data, y_data = x_data.to(device), y_data.to(device)\n",
    "for epoch in range(100000):\n",
    "    y_pred = model.forward(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    loss_list.append(loss.item())\n",
    "    epoch_list.append(epoch)\n",
    "    # 使用optimizer会收集参数\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # 自动根据梯度更新权重\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 用acc估计模型的精确度\n",
    "    if epoch%10000 == 9999:\n",
    "        # torch.where如果满足取第一个否则取第二个, 此处依据y_pred>=0.5进行二分,如果满足则取1, 否则取0\n",
    "        y_pred_label = torch.where(y_pred>=0.5,torch.tensor([1.0]).to(device),torch.tensor([0.0]).to(device))\n",
    "        # 统计两个张量里相等元素的个数, 返回值也是张量\n",
    "        acc = torch.eq(y_pred_label, y_data).sum().item()/y_data.size(0)\n",
    "        print(\"loss = \",loss.item(), \"acc = \",acc)\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "layer1_weight = model.linear1.weight.data\n",
    "layer1_bias = model.linear1.bias.data\n",
    "print(\"layer1_weight.shape\", layer1_weight.shape, \"\\nlayer1_weight\\n\", layer1_weight)\n",
    "print(\"layer1_bias.shape\", layer1_bias.shape, \"\\nlayer1_bias\", layer1_bias)\n",
    "\n",
    "plt.rcParams['font.family'] = 'SimHei'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.title(\"损失函数\")\n",
    "# plt.xticks(np.arange(0,1000000,40000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8472a43",
   "metadata": {},
   "source": [
    "# Mini_Batch处理数据集 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ff225",
   "metadata": {},
   "source": [
    "上述的*`y_pred = model.forward(x_data)`* 为梯度下降，利用了全部的数据集进行计算，可能会出现鞍点，性能低；如果使用随机梯度下降用一个样本计算会避免鞍点的问题，模型性能比较好（泛化能力），但无法利用并行性，速度慢\n",
    "\n",
    "使用mini_batch可以综合上述的问题，再追求较好的模型时不至于太慢的训练速度：\n",
    "\n",
    "-------\n",
    "三个要点：\n",
    "+ Epoch\n",
    "```python\n",
    "# Training cycle总循环次数`\n",
    "for epoch in range(training_epochs):`\n",
    "    # Loop over all batches在所有的batch上进行循环——随机梯度下降`\n",
    "    for i in range(total_batch):`\n",
    "```\n",
    "\n",
    "&emsp;外层设置轮数，内层对batch进行迭代，所有的样本都进行了一轮前馈传播和反向传播，称为一次**epoch**\n",
    "+ Batch-size\n",
    "\n",
    "&emsp;一次进行的前馈和反向所用的样本数量\n",
    "\n",
    "+ Iteration\n",
    "\n",
    "&emsp;内层循环进行的次数，——mini_batch的个数\n",
    "\n",
    "------------------\n",
    "\n",
    "*DataLoader:batch_size=2, shuffle=True* ：两个样本为一个mini_batch，并且对mini_batch进行***shuffle***打乱顺序，得到的Iterator l能执行索引并且知道长度\n",
    "<img src='./Image/Pytorch深度学习实践/生成mini_batch.png' width = 600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae6926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T09:30:58.249229Z",
     "start_time": "2023-04-10T09:30:58.160210Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]  # shape(多少行，多少列) 得到行数即数据集的大小\n",
    "        self.x_data = torch.from_numpy(xy[:, :-1]).to(device)\n",
    "        self.y_data = torch.from_numpy(xy[:, -1:]).to(device)\n",
    "\n",
    "    #魔法方法, 能使用索引来访问元素\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    #魔法方法, 能获取长度\n",
    "    def __len__(self):\n",
    "        return self.len \n",
    "    \n",
    "#准备数据集\n",
    "#-------------------------------------------------------------------------------------------    \n",
    "dataset = DiabetesDataset(\"./dataset/pytorch深度学习入门/diabetes.csv.gz\")\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "list(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ee1c4",
   "metadata": {},
   "source": [
    "可以看到，加载出来的train_loader每一项都是一个元组，元组里是一个$32样本\\times N维$的张量和一个$32样本\\times 1维$的张量，即样本特征向取值的映射，这32个样本则作为一个Mini_Batch做一次内循环供后面进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900427c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-26T14:47:38.570121Z",
     "start_time": "2023-03-26T14:40:51.774108Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]  # shape(多少行，多少列) 得到行数即数据集的大小\n",
    "        self.x_data = torch.from_numpy(xy[:, :-1]).to(device)\n",
    "        self.y_data = torch.from_numpy(xy[:, -1:]).to(device)\n",
    "\n",
    "    #魔法方法, 能使用索引来访问元素\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    #魔法方法, 能获取长度\n",
    "    def __len__(self):\n",
    "        return self.len \n",
    "    \n",
    "#准备数据集\n",
    "#-------------------------------------------------------------------------------------------    \n",
    "dataset = DiabetesDataset(\"./dataset/pytorch深度学习入门/diabetes.csv.gz\")\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "#构建模型\n",
    "#-------------------------------------------------------------------------------------------\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 2)\n",
    "        self.linear4 = torch.nn.Linear(2, 1)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Relu(self.linear1(x))\n",
    "        x = self.Relu(self.linear2(x))\n",
    "        x = self.Relu(self.linear3(x))\n",
    "        x = self.sigmoid(self.linear4(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model.to(device)\n",
    "\n",
    "#设置损失函数和优化器\n",
    "#-------------------------------------------------------------------------------------------\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "# 来自optim模块,SGD是一个优化器类,第一个参数传递需要求梯度的参数,第二个是步长\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "for epoch in range(10000):\n",
    "    losssum = 0.0\n",
    "    sum = 0\n",
    "    # 进行两层循环, 如果内存循环mini_batch大小为1, 实际上就是随机梯度下降 \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        losssum += loss.item()\n",
    "        sum += 1\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    \n",
    "    losssum /= sum\n",
    "    if epoch%1000 == 999:\n",
    "        print(losssum)\n",
    "    loss_list.append(losssum)\n",
    "    epoch_list.append(epoch)\n",
    "\n",
    "plt.rcParams['font.family'] = 'SimHei'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid()\n",
    "plt.title(\"损失函数\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d3c40",
   "metadata": {},
   "source": [
    "# 多分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb718f1",
   "metadata": {},
   "source": [
    "对于离散分布，有多个取值，一个样本对每个取值都会输出一定概率$\\sum p_i=1(p_i>0)$，从中选取概率最大的为预测值；\n",
    "\n",
    "即我们通过全连接神经网络，将样本的输入（以手写数字识别为例，28\\*28=784个像素），映射到分类数（即0-9十个分类）的10个输出，然后将这10个输出处理成概率分布，即***Softmax function***：\n",
    "$$P(y=i)=\\frac{e^{z_i}}{\\sum\\nolimits_{j=0}^{K-1}e^{z_j}},i\\in\\{0,...,K-1\\}$$\n",
    "\n",
    "从中选择概率最大的为预测值，然后通过：$LOSS(Y,\\hat Y)=-Ylog\\hat Y$给出损失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce4d07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-09T16:36:41.430473Z",
     "start_time": "2023-04-09T16:36:41.407469Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import show_output\n",
    "y = np.array([1,0,0])\n",
    "z = np.array([0.2,0.1,-0.1])\n",
    "y_pred = np.exp(z)/np.exp(z).sum()\n",
    "loss = (- y * np.log(y_pred)).sum()\n",
    "\n",
    "loss\n",
    "y_pred\n",
    "-1 * np.log(y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d69eb08",
   "metadata": {},
   "source": [
    "## softmax layer\n",
    "对于前面的层，用Sigmoid激活函数做处理，而对于最后一层则需要特殊处理，使得最后所有输出的值均为正值，并且得到的概率和为1\n",
    "\n",
    "<img src='./Image/Pytorch深度学习实践/softmax.png' width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1871f10",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f898ac",
   "metadata": {},
   "source": [
    "交叉熵损失，神经网络最后一层不做非线性变换，而是直接通过CrossEntropyLoss层，得到最后的概率分布\n",
    "<img src='./Image/Pytorch深度学习实践/CrossEntropyLoss损失.png' width = 600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f6f02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T13:04:29.197729Z",
     "start_time": "2023-04-25T13:04:25.315905Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "y = torch.LongTensor([0])\n",
    "z = torch.Tensor([[0.2,0.1,-0.1]]) # 1个样本*3维特征\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(z,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e693bf",
   "metadata": {},
   "source": [
    "CrossEntropyLoss损失$<=>$Softmax + Log + NLLLoss\n",
    "\n",
    "$$NLL(log(softmax(input)),target)=-\\sum\\nolimits_{i=1}^nOneHot(target)_i\\times log(softmax(input)_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d943eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T13:57:49.074240Z",
     "start_time": "2023-05-04T13:57:46.783753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss1 = tensor(0.4966) \n",
      "Batch Loss2 = tensor(1.2389)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "Y = torch.LongTensor([2,0,1])\n",
    "Y_pred1 = torch.Tensor([[0.1,0.2,0.9], # 2\n",
    "                        [1.1,0.1,0.2], # 0\n",
    "                        [0.2,2.1,0.1]])# 1\n",
    "\n",
    "Y_pred2 = torch.Tensor([[0.8,0.2,0.3], # 0\n",
    "                        [0.2,0.3,0.5], # 2\n",
    "                        [0.2,0.2,0.5]])# 2\n",
    "\n",
    "l1 = criterion(Y_pred1, Y)\n",
    "l2 = criterion(Y_pred2, Y)\n",
    "print(\"Batch Loss1 =\", l1.data, \"\\nBatch Loss2 =\", l2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab99c6",
   "metadata": {},
   "source": [
    "由下面的程序可以看出，最后得到的CrossEntropyLoss损失，是先对输出Y_pred求softmax后取log，然后挑出其中和样本取值对应的下标的值，然后取负求和求平均（即label的y值是onehot的，而不是带入其本身类别号计算）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76f72d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T14:00:09.467498Z",
     "start_time": "2023-05-04T14:00:09.453495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.76794955, -1.36794955, -1.26794955],\n",
       "        [-1.23983106, -1.13983106, -0.93983106],\n",
       "        [-1.2089182 , -1.2089182 , -0.9089182 ]]),\n",
       " 1.2388996025682044,\n",
       " -1.2388996025682044)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y = np.array([2,0,1])\n",
    "Y_pred = np.array([[0.8,0.2,0.3], # 0\n",
    "                   [0.2,0.3,0.5], # 2\n",
    "                   [0.2,0.2,0.5]])# 2\n",
    "Y_softmax =np.exp(Y_pred) / np.sum(np.exp(Y_pred),axis=1).reshape(-1,1) # axis=0按列求和, axis=1按行求和\n",
    "Y_log = np.log(Y_softmax)\n",
    "Y_log, -np.array([Y_log[i][Y[i]] for i in range(0,3)]).sum()/len(Y), (Y_log[0][2] + Y_log[1][0] + Y_log[2][1])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5faf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T03:34:02.806150Z",
     "start_time": "2023-04-10T03:34:02.797148Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[4,6,8],[3,6,9]])\n",
    "y = np.array([[2],[3]])\n",
    "x/y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b9212",
   "metadata": {},
   "source": [
    "## 手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc78ae2",
   "metadata": {},
   "source": [
    "对于一张图片，它由（通道数channel，宽w，高h）组成，在数据集中，图片表示为$W\\times H\\times C$，而在pytorch里，图片表示为$C\\times W\\times H$，需要做一定处理：对于一张灰度图，它的通道数为1，而对于一张彩色图片，它的通道数为3（RGB），在MNIST手写图片数据集中，每个像素都从$(0,255)$取值；\n",
    "\n",
    "由于神经网络喜欢处理0-1的数据，故先需要将其进行转换，并且，将其映射到标准正态分布上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd535c2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:35:33.845516Z",
     "start_time": "2023-04-10T08:35:33.828512Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                   # 将数据映射到0-1上\n",
    "    transforms.Normalize((0.1307,),(0.3801,))# 使用正态分布做归一化处理\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c568e",
   "metadata": {},
   "source": [
    "对于全连接神经网络，处理的是多维特征而不是矩阵，故需要将$N\\times1\\times28\\times$28的矩阵做扁平化处理，将其变为$N\\times784$的矩阵，这样它的每一行就可以对应一个特征取值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfb318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:42:27.762600Z",
     "start_time": "2023-04-10T08:42:27.745596Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[[1,2,3],[4,5,6],[7,8,9]],         # 第一个3*3的矩阵\n",
    "              [[10,11,12],[13,14,15],[16,17,18]] # 第二个3*3的矩阵\n",
    "             ])\n",
    "x.reshape(-1,9) # 做扁平化处理, 将每个矩阵拉成9维特征的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc94755",
   "metadata": {},
   "source": [
    "### 获取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74280694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T09:09:35.254662Z",
     "start_time": "2023-04-10T09:09:35.197650Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取MNIST数据集\n",
    "import torchvision\n",
    "# download = true, 没有该数据集则从网上下载下来\n",
    "train_set = torchvision.datasets.MNIST(root='./dataset/pytorch深度学习入门/mnist',\n",
    "                                       train=True,\n",
    "                                       download=True)\n",
    "train_set = torchvision.datasets.MNIST(root='./dataset/pytorch深度学习入门/mnist',\n",
    "                                       train=False,\n",
    "                                       download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f42aa5",
   "metadata": {},
   "source": [
    "### 具体代码\n",
    "不做归一化分布处理——`transforms.Normalize((0.1307,), (0.3081,))`，会使得收敛变慢，但最后结果近似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe22bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T16:04:17.527694Z",
     "start_time": "2023-05-02T16:04:17.438934Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#准备数据集\n",
    "#-------------------------------------------------------------------------------------------    \n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # 归一化,均值和方差\n",
    "\n",
    "# 上一节我们使用DataSet自行构造数据集, 而此处使用torchvision提供好的数据集\n",
    "# 该datasets里面init, getitem, len魔法函数已实现。\n",
    "train_dataset = datasets.MNIST(root='./dataset/pytorch深度学习入门/mnist', train=True, download=True, transform=transform) # 训练集\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./dataset/pytorch深度学习入门/mnist', train=False, download=True, transform=transform) # 测试集\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) # 测试集没有必要做洗牌操作打乱顺序\n",
    "\n",
    "def trainTest(model, cycles = 10):\n",
    "    #设置损失函数和优化器\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # 来自optim模块,SGD是一个优化器类,第一个参数接收当前经过梯度递减后的参数,第二个是步长\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    \n",
    "    #循环优化\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #封装一轮训练集\n",
    "    def train(epoch):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, data in enumerate(train_loader,0):\n",
    "            inputs, target = data\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forword前馈 + backward反馈 + update更新\n",
    "\n",
    "            #前馈\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs,target)\n",
    "\n",
    "            #反馈\n",
    "            loss.backward() # loss进行backward后，实际上是更新模型里权重的梯度\n",
    "\n",
    "            #优化\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 300 == 299:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    def test():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # 进行验证, 不需要进行梯度\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, dim=1) # dim = 1按行取最大值, 返回最大值的值和下标的元组\n",
    "                total += labels.size(0) # 取0表示个数, 取1表示维数\n",
    "                correct += (predicted == labels).sum().item() # 比较张量, 统计为真的值, 返回个数的标量\n",
    "        print('accuracy on test set: %d %% ' % (100*correct/total))        \n",
    "\n",
    "    \n",
    "    for epoch in range(cycles):\n",
    "        train(epoch) # 将model作为参数传入可能会有问题\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18320e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T16:07:10.848345Z",
     "start_time": "2023-05-02T16:04:18.906430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 2.247\n",
      "[1,   600] loss: 1.061\n",
      "[1,   900] loss: 0.427\n",
      "accuracy on test set: 90 % \n",
      "[2,   300] loss: 0.310\n",
      "[2,   600] loss: 0.267\n",
      "[2,   900] loss: 0.225\n",
      "accuracy on test set: 94 % \n",
      "[3,   300] loss: 0.178\n",
      "[3,   600] loss: 0.167\n",
      "[3,   900] loss: 0.157\n",
      "accuracy on test set: 95 % \n",
      "[4,   300] loss: 0.128\n",
      "[4,   600] loss: 0.121\n",
      "[4,   900] loss: 0.111\n",
      "accuracy on test set: 96 % \n",
      "[5,   300] loss: 0.096\n",
      "[5,   600] loss: 0.092\n",
      "[5,   900] loss: 0.089\n",
      "accuracy on test set: 96 % \n",
      "[6,   300] loss: 0.071\n",
      "[6,   600] loss: 0.073\n",
      "[6,   900] loss: 0.074\n",
      "accuracy on test set: 97 % \n",
      "[7,   300] loss: 0.062\n",
      "[7,   600] loss: 0.056\n",
      "[7,   900] loss: 0.058\n",
      "accuracy on test set: 97 % \n",
      "[8,   300] loss: 0.048\n",
      "[8,   600] loss: 0.044\n",
      "[8,   900] loss: 0.048\n",
      "accuracy on test set: 97 % \n",
      "[9,   300] loss: 0.039\n",
      "[9,   600] loss: 0.037\n",
      "[9,   900] loss: 0.038\n",
      "accuracy on test set: 97 % \n",
      "[10,   300] loss: 0.030\n",
      "[10,   600] loss: 0.028\n",
      "[10,   900] loss: 0.033\n",
      "accuracy on test set: 97 % \n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "#-------------------------------------------------------------------------------------------\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(784, 512)\n",
    "        self.l2 = torch.nn.Linear(512, 256)\n",
    "        self.l3 = torch.nn.Linear(256, 128)\n",
    "        self.l4 = torch.nn.Linear(128, 64)\n",
    "        self.l5 = torch.nn.Linear(64, 10)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)   # -1做扁平化操作\n",
    "        x = self.Relu(self.l1(x))\n",
    "        x = self.Relu(self.l2(x))\n",
    "        x = self.Relu(self.l3(x))\n",
    "        x = self.Relu(self.l4(x))\n",
    "        return self.l5(x)    # 最后一层不做激活, 返回线性变换, 用于后续CrossEntropyLoss损失处理\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)\n",
    "trainTest(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53276a9b",
   "metadata": {},
   "source": [
    "python的参数传递，当参数非基本类型时，传递的为一个地址形参，可以对地址指向的内容进行修改，而直接对形参地址赋值是不会修改原有值的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2357fa73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T16:01:00.616839Z",
     "start_time": "2023-05-02T16:01:00.605903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [5, 2, 3], [1, 2, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1\n",
    "y = [1, 2, 3]\n",
    "z = [1, 2, 3]\n",
    "\n",
    "def changex(x):\n",
    "    x = 1\n",
    "def changey(y):\n",
    "    y[0] = 5\n",
    "def changez(z):\n",
    "    z = [2,3,4]\n",
    "    \n",
    "changex(x)\n",
    "changey(y)\n",
    "changez(z)\n",
    "\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9e5c0",
   "metadata": {},
   "source": [
    "# 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c660e7",
   "metadata": {},
   "source": [
    "Convolutional Netural Network —— CNN\n",
    "\n",
    "之前的全连接神经网络，对于每两个网络层之间的任意两个点，都有相应的权重，如果一个输入大小为100\\*100的图像层，映射到100\\*100的隐藏层，那么这两个层之间的$\\omega$的个数就达到了$100^4$个，考虑用32精度4字节浮点数表示，有$4*10^8$个字节约为400MB，太过庞大；\n",
    "\n",
    "并且在全连接神经网络中，对于一张图像矩阵，是直接做扁平化处理拉成一维向量，丢失了很多信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b92d8",
   "metadata": {},
   "source": [
    "## 卷积核"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79efc3",
   "metadata": {},
   "source": [
    "我们假设一组图像数据的属性为（B,C,W,H）—（batch,channel,宽,高），那么对于该图像的每一个channel，都应该有一个卷积核，对于原图像的每一块卷积核大小的区域，都按位与卷积核做数乘，然后求和，得到该区域经过卷积后的值\n",
    "\n",
    "一个5\\*5大小的图像，经过3\\*3的卷积核，变成3\\*3的图像：\n",
    "<img src='./Image/Pytorch深度学习实践/单通道卷积.png' width = 600>\n",
    "对于每一个输入通道，我们都应该有一个相应的卷积核，对于由这样一组卷积核进行卷积运算得到的结果（$C\\times\\overline W\\times\\overline H$）做加法，得到一个输出通道的结果——一张*'C'* 输入多通道图片经过一组*'C'* 个卷积核运算得到一个输出通道\n",
    "+ 一组卷积操作：\n",
    "<img src='./Image/Pytorch深度学习实践/一组卷积操作.png' width = 600>\n",
    "\n",
    "如果我们想得到M个输出通道，那么我们就需要M组卷积核，这样原来的一组图片：$B\\times C_{in}\\times W_{in}\\times H_{in}$变成$B\\times\\overline C_{out}\\times\\overline W_{out}\\times\\overline H_{out}$，最后的$W/H_{out}$由$W/H_{in}$和卷积核大小共同决定：3\\*3的卷积核原大小-2，5\\*5的卷积核原大小-4\n",
    "+ m组卷积操作：\n",
    "<img src='./Image/Pytorch深度学习实践/m组卷积操作.png' width = 600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff65f40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:21:48.194552Z",
     "start_time": "2023-04-12T16:21:48.182840Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "in_channels,out_channels= 5,10 # 输入通道数, 输出通道数\n",
    "width,height = 100, 100 # 宽, 高\n",
    "kernel_size = 3 # 卷积核大小\n",
    "batch_size = 1\n",
    "input = torch.randn(batch_size,\n",
    "                    in_channels,\n",
    "                    width,\n",
    "                    height)\n",
    "\n",
    "conv_layer = torch.nn.Conv2d(in_channels,\n",
    "                             out_channels,\n",
    "                             kernel_size=kernel_size)\n",
    "\n",
    "output = conv_layer(input)\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "print(conv_layer.weight.shape) # 10*5*3*3, 10组卷积核, 每组卷积核有5个卷积核=输入通道数, 3*3的卷积核大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c1d0f",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969071a",
   "metadata": {},
   "source": [
    "将图像外层添加一圈（默认）0，使得图像经过3\\*3的卷积核后大小不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df09d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-12T16:24:16.925383Z",
     "start_time": "2023-04-12T16:24:16.916600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "in_channels,out_channels= 5,10\n",
    "\n",
    "input = [3,4,6,5,7,\n",
    "         2,4,6,8,2,\n",
    "         1,6,7,8,4,\n",
    "         9,7,4,6,2,\n",
    "         3,7,5,4,1]\n",
    "\n",
    "input = torch.Tensor(input).view(1,1,5,5) # b = 1, c = 1, w = 5, h = 5\n",
    "\n",
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "kernel = torch.Tensor([1,2,3,4,5,6,7,8,9]).view(1, 1, 3, 3) # m/output_channel = 1, c/input_channel = 1, w = 3, h = 3\n",
    "conv_layer.weight.data = kernel.data # 设置卷积核的权重\n",
    "\n",
    "output = conv_layer(input)\n",
    "print(output, output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537d25d",
   "metadata": {},
   "source": [
    "## 池化层pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02456499",
   "metadata": {},
   "source": [
    "以MaxPooling为例，设池化层的大小为2（2\\*2）,则对每一个通道按2\\*2方阵，默认步长stride=2做处理，选取每一个小方阵里最大的数；通过池化层减少数据的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c08d6",
   "metadata": {},
   "source": [
    "## 通过卷积对图像进行处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69abda3",
   "metadata": {},
   "source": [
    "对最后的卷积结果，直接扁平化操作(flatten)，将线性特征做全连接映射到10维的输出，做softmax得到概率取值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cfa37ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T14:13:58.526716Z",
     "start_time": "2023-05-02T14:04:52.495512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.967\n",
      "[1,   600] loss: 0.233\n",
      "[1,   900] loss: 0.156\n",
      "accuracy on test set: 96 % \n",
      "[2,   300] loss: 0.119\n",
      "[2,   600] loss: 0.100\n",
      "[2,   900] loss: 0.095\n",
      "accuracy on test set: 97 % \n",
      "[3,   300] loss: 0.081\n",
      "[3,   600] loss: 0.074\n",
      "[3,   900] loss: 0.068\n",
      "accuracy on test set: 98 % \n",
      "[4,   300] loss: 0.059\n",
      "[4,   600] loss: 0.061\n",
      "[4,   900] loss: 0.062\n",
      "accuracy on test set: 98 % \n",
      "[5,   300] loss: 0.051\n",
      "[5,   600] loss: 0.050\n",
      "[5,   900] loss: 0.055\n",
      "accuracy on test set: 98 % \n",
      "[6,   300] loss: 0.044\n",
      "[6,   600] loss: 0.044\n",
      "[6,   900] loss: 0.047\n",
      "accuracy on test set: 98 % \n",
      "[7,   300] loss: 0.040\n",
      "[7,   600] loss: 0.038\n",
      "[7,   900] loss: 0.040\n",
      "accuracy on test set: 98 % \n",
      "[8,   300] loss: 0.037\n",
      "[8,   600] loss: 0.039\n",
      "[8,   900] loss: 0.034\n",
      "accuracy on test set: 98 % \n",
      "[9,   300] loss: 0.032\n",
      "[9,   600] loss: 0.030\n",
      "[9,   900] loss: 0.035\n",
      "accuracy on test set: 98 % \n",
      "[10,   300] loss: 0.029\n",
      "[10,   600] loss: 0.030\n",
      "[10,   900] loss: 0.030\n",
      "accuracy on test set: 98 % \n"
     ]
    }
   ],
   "source": [
    "#构建模型\n",
    "#-------------------------------------------------------------------------------------------\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,10,kernel_size=5) # 1个输入通道, 10个输出通道的卷积核\n",
    "        self.conv2 = torch.nn.Conv2d(10,20,kernel_size=5) # 10个输入通道, 20个输出通道的卷积核\n",
    "        self.pooling = torch.nn.MaxPool2d(2) # 2*2的方阵, 步长为2做池化\n",
    "        self.l1 = torch.nn.Linear(320, 64)\n",
    "        self.l2 = torch.nn.Linear(64, 10)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.pooling(self.Relu(self.conv1(x)))\n",
    "        x = self.pooling(self.Relu(self.conv2(x)))\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.Relu(self.l1(x))\n",
    "        return self.l2(x)    # 最后一层不做激活, 返回线性变换, 用于后续CrossEntropyLoss损失处理\n",
    "    \n",
    "model = Net()\n",
    "\n",
    "trainTest(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c98403",
   "metadata": {},
   "source": [
    "# CNN的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5f4bd",
   "metadata": {},
   "source": [
    "减少代码冗余：模型中的重复部分可以抽象出来单独作为一个模块，以便重复利用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0f871",
   "metadata": {},
   "source": [
    "## Inception Module\n",
    "将一些卷积核放在一起组成一个模块，经过训练，某些模块的参数会变大（即模型更倾向于选择该模块），从而减少了人为选择模块带来的误差\n",
    "\n",
    "### 1$\\times$1的卷积核：\n",
    "\n",
    "将图像的三个通道通过1$\\times$1的卷积核处理，可以达到对通道进行混合的效果：①例如对RGB三色通道结果$\\sum \\omega_i=1$进行处理，可以提高某个通道的权重；（每个通道都需要一个1$\\times$1的卷积核$\\omega_i$）②或者都乘以$\\omega=1$的卷积核，相当于求总值\n",
    "\n",
    "同时，1$\\times$1卷积核也可以用于改变原输入的通道数，为后续的$kernel>1$的卷积运算减少运算量\n",
    "\n",
    "<img src='./Image/Pytorch深度学习实践/1-1卷积核.png' width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e93a73",
   "metadata": {},
   "source": [
    "### Concatenate连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bfa9f",
   "metadata": {},
   "source": [
    "将不同的张量，沿着某一通道进行拼接，例如$2\\times4\\times4$和$3\\times4\\times4$的张量拼接到一起，变成$5\\times4\\times4$的张量，想要对通过不同的卷积核出来的结果进行拼接，首先就要求输出图像的宽高都是一样的，即允许变换的只是图像的通道数Channel（几组卷积核就几个channel），这就要求对图像做处理是要添加padding，使得图像的大小不发生变换\n",
    "\n",
    "<img src='./Image/Pytorch深度学习实践/Inception.png' width = 400>\n",
    "\n",
    "可以看到，示例的Inception网络，最后会由四个不同通道数的块Concatenate拼接而成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25906f2a",
   "metadata": {},
   "source": [
    "拼接的张量，必须除了拼接维度上不同以外，其它维度均相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7492646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:26:47.518445Z",
     "start_time": "2023-05-02T15:26:47.510675Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]]),\n",
       " tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]]),\n",
       " tensor([[[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]],\n",
       " \n",
       "         [[0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.],\n",
       "          [0., 0.]]]),\n",
       " torch.Size([2, 8, 2]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((2,3,2))\n",
    "y = torch.zeros((2,5,2))\n",
    "z = torch.cat((x, y), dim = 1)\n",
    "x, y, z, z.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731fedb3",
   "metadata": {},
   "source": [
    "在Concatenate里，（B,C,W,H）中是沿着C进行拼接的，即dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9b421",
   "metadata": {},
   "source": [
    "### 代码实现 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bc9ce9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T17:18:48.066442Z",
     "start_time": "2023-05-02T17:13:13.418908Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 2.102\n",
      "[1,   600] loss: 0.468\n",
      "[1,   900] loss: 0.303\n",
      "accuracy on test set: 91 % \n",
      "[2,   300] loss: 0.233\n",
      "[2,   600] loss: 0.207\n",
      "[2,   900] loss: 0.178\n",
      "accuracy on test set: 94 % \n",
      "[3,   300] loss: 0.160\n",
      "[3,   600] loss: 0.149\n",
      "[3,   900] loss: 0.138\n",
      "accuracy on test set: 96 % \n",
      "[4,   300] loss: 0.123\n",
      "[4,   600] loss: 0.121\n",
      "[4,   900] loss: 0.124\n",
      "accuracy on test set: 96 % \n",
      "[5,   300] loss: 0.107\n",
      "[5,   600] loss: 0.105\n",
      "[5,   900] loss: 0.111\n",
      "accuracy on test set: 96 % \n",
      "[6,   300] loss: 0.096\n",
      "[6,   600] loss: 0.094\n",
      "[6,   900] loss: 0.094\n",
      "accuracy on test set: 97 % \n",
      "[7,   300] loss: 0.084\n",
      "[7,   600] loss: 0.082\n",
      "[7,   900] loss: 0.082\n",
      "accuracy on test set: 97 % \n",
      "[8,   300] loss: 0.078\n",
      "[8,   600] loss: 0.077\n",
      "[8,   900] loss: 0.073\n",
      "accuracy on test set: 97 % \n",
      "[9,   300] loss: 0.065\n",
      "[9,   600] loss: 0.067\n",
      "[9,   900] loss: 0.067\n",
      "accuracy on test set: 98 % \n",
      "[10,   300] loss: 0.062\n",
      "[10,   600] loss: 0.061\n",
      "[10,   900] loss: 0.056\n",
      "accuracy on test set: 98 % \n"
     ]
    }
   ],
   "source": [
    "#构建模型, 构造出一个通用模块InceptionA\n",
    "#-------------------------------------------------------------------------------------------\n",
    "class InceptionA(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.avgpool_1 = torch.nn.AvgPool2d(kernel_size = 3, stride = 1,padding = 1)\n",
    "        self.avgpool_2 = torch.nn.Conv2d(in_channels, 24, kernel_size = 1)\n",
    "        \n",
    "        self.conv1x1 = torch.nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        \n",
    "        self.conv5x5_1 = torch.nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        self.conv5x5_2 = torch.nn.Conv2d(16, 24, kernel_size = 1)\n",
    "        \n",
    "        self.conv3x3_1 = torch.nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        self.conv3x3_2 = torch.nn.Conv2d(16, 24, kernel_size = 1)\n",
    "        self.conv3x3_3 = torch.nn.Conv2d(24, 24, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch_avg = self.avgpool_1(x)\n",
    "        branch_avg = self.avgpool_2(branch_avg)\n",
    "        \n",
    "        branch_1x1 = self.conv1x1(x)\n",
    "        \n",
    "        branch_5x5 = self.conv5x5_1(x)\n",
    "        branch_5x5 = self.conv5x5_2(branch_5x5)\n",
    "        \n",
    "        branch_3x3 = self.conv3x3_1(x)\n",
    "        branch_3x3 = self.conv3x3_2(branch_3x3)\n",
    "        branch_3x3 = self.conv3x3_3(branch_3x3)\n",
    "        \n",
    "        # 最后输出的channel数量为24 + 16 + 24 + 24 = 88\n",
    "        return torch.cat((branch_avg, branch_1x1, branch_5x5, branch_3x3), dim = 1)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,10,kernel_size=5) # 1个输入通道, 10个输出通道的卷积核\n",
    "        self.conv2 = torch.nn.Conv2d(88,20,kernel_size=5) # 10个输入通道, 20个输出通道的卷积核\n",
    "        self.pooling = torch.nn.AvgPool2d(kernel_size = 3, padding = 1) # 3 * 3的方阵做池化\n",
    "        \n",
    "        self.incep1 = InceptionA(in_channels=10)\n",
    "        self.incep2 = InceptionA(in_channels=20)\n",
    "        \n",
    "        self.l1 = torch.nn.Linear(320, 64)\n",
    "        self.l2 = torch.nn.Linear(64, 10)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.pooling(self.Relu(self.conv1(x)))\n",
    "        x = self.pooling(self.Relu(self.conv2(x)))\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.Relu(self.l1(x))\n",
    "        return self.l2(x)    # 最后一层不做激活, 返回线性变换, 用于后续CrossEntropyLoss损失处理\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)\n",
    "trainTest(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b00b8b",
   "metadata": {},
   "source": [
    "## resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdedb683",
   "metadata": {},
   "source": [
    "Residual Net —— 残差网络\n",
    "\n",
    "在实际运用中，神经网络的效率有可能并不会随着层数的增加而提高——反而有可能下降。其主要原因是，随着神经网络层数的增多，在靠近输入端的梯度随着连乘，可能会趋近于0，而权重的更新公式为：$\\omega = \\omega - \\alpha*grad$，使得权重不会得到有效的更新，最后出现拟合效果差的局面——靠近输入端的权重几乎不变\n",
    "\n",
    "<img src='./Image/Pytorch深度学习实践/resnet.png' width = 500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "602bf279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T18:11:42.824428Z",
     "start_time": "2023-05-02T18:11:42.804483Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        # 输入通道和输出通道相同\n",
    "        self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1) \n",
    "        self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1) \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.conv1(x))\n",
    "        y = self.relu(self.conv2(y))\n",
    "        return self.relu(y + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21975e",
   "metadata": {},
   "source": [
    "残差网络可以单独作为一个层穿插在神经网络中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3e15b",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0709ba86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T18:34:47.174863Z",
     "start_time": "2023-05-02T18:31:49.893991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   300] loss: 0.858\n",
      "[1,   600] loss: 0.214\n",
      "[1,   900] loss: 0.150\n",
      "accuracy on test set: 96 % \n",
      "[2,   300] loss: 0.110\n",
      "[2,   600] loss: 0.097\n",
      "[2,   900] loss: 0.087\n",
      "accuracy on test set: 97 % \n",
      "[3,   300] loss: 0.077\n",
      "[3,   600] loss: 0.068\n",
      "[3,   900] loss: 0.064\n",
      "accuracy on test set: 98 % \n",
      "[4,   300] loss: 0.056\n",
      "[4,   600] loss: 0.057\n",
      "[4,   900] loss: 0.055\n",
      "accuracy on test set: 98 % \n",
      "[5,   300] loss: 0.044\n",
      "[5,   600] loss: 0.051\n",
      "[5,   900] loss: 0.048\n",
      "accuracy on test set: 98 % \n",
      "[6,   300] loss: 0.039\n",
      "[6,   600] loss: 0.040\n",
      "[6,   900] loss: 0.042\n",
      "accuracy on test set: 98 % \n",
      "[7,   300] loss: 0.035\n",
      "[7,   600] loss: 0.033\n",
      "[7,   900] loss: 0.039\n",
      "accuracy on test set: 98 % \n",
      "[8,   300] loss: 0.030\n",
      "[8,   600] loss: 0.032\n",
      "[8,   900] loss: 0.033\n",
      "accuracy on test set: 98 % \n",
      "[9,   300] loss: 0.027\n",
      "[9,   600] loss: 0.031\n",
      "[9,   900] loss: 0.029\n",
      "accuracy on test set: 98 % \n",
      "[10,   300] loss: 0.029\n",
      "[10,   600] loss: 0.024\n",
      "[10,   900] loss: 0.026\n",
      "accuracy on test set: 98 % \n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1,10,kernel_size=5) # 1个输入通道, 10个输出通道的卷积核\n",
    "        self.conv2 = torch.nn.Conv2d(10,20,kernel_size=5) # 10个输入通道, 20个输出通道的卷积核\n",
    "        self.pooling = torch.nn.MaxPool2d(2) # 2 * 2的方阵, 步长为2做池化\n",
    "        \n",
    "        self.rblock1 = ResidualBlock(channels=10)\n",
    "        self.rblock2 = ResidualBlock(channels=20)\n",
    "        \n",
    "        self.l1 = torch.nn.Linear(320, 64)\n",
    "        self.l2 = torch.nn.Linear(64, 10)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.pooling(self.Relu(self.conv1(x)))\n",
    "        x = self.rblock1(x)\n",
    "        x = self.pooling(self.Relu(self.conv2(x)))\n",
    "        x = self.rblock2(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.Relu(self.l1(x))\n",
    "        return self.l2(x)    # 最后一层不做激活, 返回线性变换, 用于后续CrossEntropyLoss损失处理\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)\n",
    "trainTest(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2155d",
   "metadata": {},
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346c4cd",
   "metadata": {},
   "source": [
    "RNN —— Recurrent Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cpu]",
   "language": "python",
   "name": "conda-env-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
